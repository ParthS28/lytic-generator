{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils as ku\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('dataset.txt', encoding = 'utf8').read()\n",
    "corpus = data.lower().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 17, 100)           152300    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 17, 300)           301200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 17, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 761)               76861     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1523)              1160526   \n",
      "=================================================================\n",
      "Total params: 1,851,287\n",
      "Trainable params: 1,851,287\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/200\n",
      "6588/6588 [==============================] - 18s 3ms/sample - loss: 6.6444 - acc: 0.0328\n",
      "Epoch 2/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 5.8991 - acc: 0.0345\n",
      "Epoch 3/200\n",
      "6588/6588 [==============================] - 16s 2ms/sample - loss: 5.7335 - acc: 0.0381\n",
      "Epoch 4/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 5.6046 - acc: 0.0455\n",
      "Epoch 5/200\n",
      "6588/6588 [==============================] - 16s 2ms/sample - loss: 5.4902 - acc: 0.0478\n",
      "Epoch 6/200\n",
      "6588/6588 [==============================] - 16s 2ms/sample - loss: 5.3827 - acc: 0.0560\n",
      "Epoch 7/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 5.2970 - acc: 0.0563\n",
      "Epoch 8/200\n",
      "6588/6588 [==============================] - 18s 3ms/sample - loss: 5.2126 - acc: 0.0654\n",
      "Epoch 9/200\n",
      "6588/6588 [==============================] - 18s 3ms/sample - loss: 5.1443 - acc: 0.0686\n",
      "Epoch 10/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 5.0638 - acc: 0.0803\n",
      "Epoch 11/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 4.9875 - acc: 0.0833\n",
      "Epoch 12/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 4.9121 - acc: 0.0973\n",
      "Epoch 13/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 4.8582 - acc: 0.1009\n",
      "Epoch 14/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 4.7703 - acc: 0.1123\n",
      "Epoch 15/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 4.6890 - acc: 0.1226\n",
      "Epoch 16/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 4.5958 - acc: 0.1240\n",
      "Epoch 17/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 4.5159 - acc: 0.1387\n",
      "Epoch 18/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 4.4357 - acc: 0.1425\n",
      "Epoch 19/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 4.3471 - acc: 0.1574\n",
      "Epoch 20/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 4.2619 - acc: 0.1688\n",
      "Epoch 21/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 4.1837 - acc: 0.1817\n",
      "Epoch 22/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 4.0990 - acc: 0.1938\n",
      "Epoch 23/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 4.0202 - acc: 0.2008\n",
      "Epoch 24/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 3.9394 - acc: 0.2165\n",
      "Epoch 25/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 3.8590 - acc: 0.2277\n",
      "Epoch 26/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 3.7742 - acc: 0.2508\n",
      "Epoch 27/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 3.7056 - acc: 0.2568\n",
      "Epoch 28/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 3.6407 - acc: 0.2665\n",
      "Epoch 29/200\n",
      "6588/6588 [==============================] - 18s 3ms/sample - loss: 3.5638 - acc: 0.2761\n",
      "Epoch 30/200\n",
      "6588/6588 [==============================] - 15s 2ms/sample - loss: 3.5149 - acc: 0.2911\n",
      "Epoch 31/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 3.4840 - acc: 0.2908\n",
      "Epoch 32/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 3.3805 - acc: 0.3106\n",
      "Epoch 33/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 3.3159 - acc: 0.3197\n",
      "Epoch 34/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 3.2542 - acc: 0.3312\n",
      "Epoch 35/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 3.2359 - acc: 0.3336\n",
      "Epoch 36/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 3.1438 - acc: 0.3514\n",
      "Epoch 37/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 3.0596 - acc: 0.3681\n",
      "Epoch 38/200\n",
      "6588/6588 [==============================] - 15s 2ms/sample - loss: 3.0128 - acc: 0.3789\n",
      "Epoch 39/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.9708 - acc: 0.3828\n",
      "Epoch 40/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.9245 - acc: 0.3906\n",
      "Epoch 41/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.8461 - acc: 0.4068\n",
      "Epoch 42/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.8131 - acc: 0.4136\n",
      "Epoch 43/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.7612 - acc: 0.4236\n",
      "Epoch 44/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.7002 - acc: 0.4359\n",
      "Epoch 45/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.6491 - acc: 0.4490\n",
      "Epoch 46/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.6100 - acc: 0.4554\n",
      "Epoch 47/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.5735 - acc: 0.4640\n",
      "Epoch 48/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.5381 - acc: 0.4719\n",
      "Epoch 49/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.4834 - acc: 0.4830\n",
      "Epoch 50/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.4374 - acc: 0.4965\n",
      "Epoch 51/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.3860 - acc: 0.5085\n",
      "Epoch 52/200\n",
      "6588/6588 [==============================] - 18s 3ms/sample - loss: 2.3843 - acc: 0.5024\n",
      "Epoch 53/200\n",
      "6588/6588 [==============================] - 20s 3ms/sample - loss: 2.3013 - acc: 0.5244\n",
      "Epoch 54/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.3354 - acc: 0.5153\n",
      "Epoch 55/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.2691 - acc: 0.5337\n",
      "Epoch 56/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.2310 - acc: 0.5407\n",
      "Epoch 57/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.1993 - acc: 0.5475\n",
      "Epoch 58/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.1325 - acc: 0.5647\n",
      "Epoch 59/200\n",
      "6588/6588 [==============================] - 18s 3ms/sample - loss: 2.0823 - acc: 0.5723\n",
      "Epoch 60/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.0415 - acc: 0.5858\n",
      "Epoch 61/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 2.0232 - acc: 0.5943\n",
      "Epoch 62/200\n",
      "6588/6588 [==============================] - 18s 3ms/sample - loss: 1.9821 - acc: 0.6002\n",
      "Epoch 63/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.9813 - acc: 0.6023\n",
      "Epoch 64/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.9342 - acc: 0.6126\n",
      "Epoch 65/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.9133 - acc: 0.6173\n",
      "Epoch 66/200\n",
      "6588/6588 [==============================] - 18s 3ms/sample - loss: 1.8846 - acc: 0.6254\n",
      "Epoch 67/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.9093 - acc: 0.6267\n",
      "Epoch 68/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.9004 - acc: 0.6233\n",
      "Epoch 69/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.8052 - acc: 0.6419\n",
      "Epoch 70/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.7633 - acc: 0.6586\n",
      "Epoch 71/200\n",
      "6588/6588 [==============================] - 18s 3ms/sample - loss: 1.7582 - acc: 0.6571\n",
      "Epoch 72/200\n",
      "6588/6588 [==============================] - 16s 2ms/sample - loss: 1.7192 - acc: 0.6709\n",
      "Epoch 73/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.6756 - acc: 0.6733\n",
      "Epoch 74/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.6595 - acc: 0.6743\n",
      "Epoch 75/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.6188 - acc: 0.6913\n",
      "Epoch 76/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.6020 - acc: 0.69570s - loss: 1.6016 - acc:\n",
      "Epoch 77/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.5854 - acc: 0.6961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.5614 - acc: 0.7049\n",
      "Epoch 79/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.5372 - acc: 0.7122\n",
      "Epoch 80/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.5391 - acc: 0.7046\n",
      "Epoch 81/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.5122 - acc: 0.7163\n",
      "Epoch 82/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.5372 - acc: 0.7117\n",
      "Epoch 83/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.5382 - acc: 0.7064\n",
      "Epoch 84/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.4674 - acc: 0.7257\n",
      "Epoch 85/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.4448 - acc: 0.7350\n",
      "Epoch 86/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.4036 - acc: 0.7385\n",
      "Epoch 87/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.4425 - acc: 0.7309\n",
      "Epoch 88/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.4073 - acc: 0.7423\n",
      "Epoch 89/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.3660 - acc: 0.7539\n",
      "Epoch 90/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.3383 - acc: 0.7582\n",
      "Epoch 91/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.3520 - acc: 0.7562\n",
      "Epoch 92/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.3505 - acc: 0.7503\n",
      "Epoch 93/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.3372 - acc: 0.7535\n",
      "Epoch 94/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.4132 - acc: 0.7312\n",
      "Epoch 95/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.3304 - acc: 0.7533\n",
      "Epoch 96/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.2842 - acc: 0.7685\n",
      "Epoch 97/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.2503 - acc: 0.7779\n",
      "Epoch 98/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.2131 - acc: 0.7887\n",
      "Epoch 99/200\n",
      "6588/6588 [==============================] - 16s 2ms/sample - loss: 1.2046 - acc: 0.7892\n",
      "Epoch 100/200\n",
      "6588/6588 [==============================] - 18s 3ms/sample - loss: 1.1845 - acc: 0.7945\n",
      "Epoch 101/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.1839 - acc: 0.7937\n",
      "Epoch 102/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.1678 - acc: 0.7974\n",
      "Epoch 103/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.1500 - acc: 0.8037\n",
      "Epoch 104/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.1280 - acc: 0.8065\n",
      "Epoch 105/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.1202 - acc: 0.8051\n",
      "Epoch 106/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.1095 - acc: 0.8049\n",
      "Epoch 107/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.1089 - acc: 0.8092\n",
      "Epoch 108/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.0937 - acc: 0.8128\n",
      "Epoch 109/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.0693 - acc: 0.8215\n",
      "Epoch 110/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.0611 - acc: 0.8213\n",
      "Epoch 111/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.0513 - acc: 0.8238\n",
      "Epoch 112/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.0315 - acc: 0.8241\n",
      "Epoch 113/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.0386 - acc: 0.8204\n",
      "Epoch 114/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.0512 - acc: 0.8175\n",
      "Epoch 115/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.0326 - acc: 0.8257\n",
      "Epoch 116/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.0130 - acc: 0.8262\n",
      "Epoch 117/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 1.0096 - acc: 0.8257\n",
      "Epoch 118/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 0.9770 - acc: 0.8386\n",
      "Epoch 119/200\n",
      "6588/6588 [==============================] - 17s 3ms/sample - loss: 0.9771 - acc: 0.8389\n",
      "Epoch 120/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.9631 - acc: 0.8376\n",
      "Epoch 121/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.9950 - acc: 0.8260\n",
      "Epoch 122/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 1.0002 - acc: 0.8292\n",
      "Epoch 123/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.9691 - acc: 0.8321\n",
      "Epoch 124/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.9337 - acc: 0.8443\n",
      "Epoch 125/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.9167 - acc: 0.8493\n",
      "Epoch 126/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.9117 - acc: 0.8520\n",
      "Epoch 127/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.9062 - acc: 0.8511\n",
      "Epoch 128/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.8994 - acc: 0.8484\n",
      "Epoch 129/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.9003 - acc: 0.8512\n",
      "Epoch 130/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.9275 - acc: 0.8438\n",
      "Epoch 131/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.9081 - acc: 0.8430\n",
      "Epoch 132/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.8697 - acc: 0.8579\n",
      "Epoch 133/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.8682 - acc: 0.8528\n",
      "Epoch 134/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.8613 - acc: 0.8540\n",
      "Epoch 135/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.8575 - acc: 0.8556\n",
      "Epoch 136/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.8675 - acc: 0.8569\n",
      "Epoch 137/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.8753 - acc: 0.8505\n",
      "Epoch 138/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.8420 - acc: 0.8602\n",
      "Epoch 139/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.8633 - acc: 0.8538\n",
      "Epoch 140/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.8276 - acc: 0.8655\n",
      "Epoch 141/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.8183 - acc: 0.8645\n",
      "Epoch 142/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7965 - acc: 0.8722\n",
      "Epoch 143/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.8263 - acc: 0.8591\n",
      "Epoch 144/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.8487 - acc: 0.8546\n",
      "Epoch 145/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.8049 - acc: 0.8682\n",
      "Epoch 146/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7867 - acc: 0.8698\n",
      "Epoch 147/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7962 - acc: 0.8675\n",
      "Epoch 148/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7808 - acc: 0.8699\n",
      "Epoch 149/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7555 - acc: 0.8760\n",
      "Epoch 150/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7541 - acc: 0.8774\n",
      "Epoch 151/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7433 - acc: 0.8760\n",
      "Epoch 152/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7330 - acc: 0.8778\n",
      "Epoch 153/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7485 - acc: 0.8774\n",
      "Epoch 154/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7437 - acc: 0.8767\n",
      "Epoch 155/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7433 - acc: 0.8763\n",
      "Epoch 156/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7475 - acc: 0.8757\n",
      "Epoch 157/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7381 - acc: 0.8761\n",
      "Epoch 158/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7950 - acc: 0.8605\n",
      "Epoch 159/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7625 - acc: 0.8698\n",
      "Epoch 160/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7379 - acc: 0.8763\n",
      "Epoch 161/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7176 - acc: 0.8808\n",
      "Epoch 162/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7788 - acc: 0.8663\n",
      "Epoch 163/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7930 - acc: 0.8600\n",
      "Epoch 164/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7520 - acc: 0.8676\n",
      "Epoch 165/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7276 - acc: 0.8752\n",
      "Epoch 166/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6982 - acc: 0.8799\n",
      "Epoch 167/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6905 - acc: 0.8839\n",
      "Epoch 168/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6761 - acc: 0.89072\n",
      "Epoch 169/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6877 - acc: 0.8830\n",
      "Epoch 170/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7086 - acc: 0.8752\n",
      "Epoch 171/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7207 - acc: 0.8752\n",
      "Epoch 172/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6907 - acc: 0.8815\n",
      "Epoch 173/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6869 - acc: 0.8833\n",
      "Epoch 174/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6662 - acc: 0.8881\n",
      "Epoch 175/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6755 - acc: 0.8854\n",
      "Epoch 176/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6607 - acc: 0.8857\n",
      "Epoch 177/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6635 - acc: 0.8848\n",
      "Epoch 178/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6625 - acc: 0.8836\n",
      "Epoch 179/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6625 - acc: 0.8846\n",
      "Epoch 180/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7713 - acc: 0.8581\n",
      "Epoch 181/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.7396 - acc: 0.8663\n",
      "Epoch 182/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6820 - acc: 0.8833\n",
      "Epoch 183/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6612 - acc: 0.8886\n",
      "Epoch 184/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6488 - acc: 0.8877\n",
      "Epoch 185/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6388 - acc: 0.8927\n",
      "Epoch 186/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6360 - acc: 0.8907\n",
      "Epoch 187/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6310 - acc: 0.8922\n",
      "Epoch 188/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6270 - acc: 0.8898\n",
      "Epoch 189/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6297 - acc: 0.8910\n",
      "Epoch 190/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6361 - acc: 0.8900\n",
      "Epoch 191/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6345 - acc: 0.8887\n",
      "Epoch 192/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6190 - acc: 0.8916\n",
      "Epoch 193/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6160 - acc: 0.8927\n",
      "Epoch 194/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6222 - acc: 0.8900\n",
      "Epoch 195/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6200 - acc: 0.8871\n",
      "Epoch 196/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6144 - acc: 0.8915\n",
      "Epoch 197/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6095 - acc: 0.8910\n",
      "Epoch 198/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6407 - acc: 0.8834\n",
      "Epoch 199/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6249 - acc: 0.8860\n",
      "Epoch 200/200\n",
      "6588/6588 [==============================] - 14s 2ms/sample - loss: 0.6229 - acc: 0.8880\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(predictors, label, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You already know things treated me ma just other i 24 phony dave thanks to be doc or man at one car daly or change it so like are won't it got my antidote up won't fuck that her she's motherfucking bag her class what she got that old me ma take you normal god got red day i don't deserve so it need each could does me him hood she freaks working away man 'cause we won't it what what her tension older thank you walk can won't treated could could it only working is one picture me in stand up write\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'You already know'\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take that imitating not provide it's you was a mothafuckin' thing but i get hell ridiculous now me long as you won't said shady i just dream i could second me man want to say to beef like me on me in you give me some ass gone day i'd wondering when you are you but one day shady i call me when call a rescued me from it's some company he choked food food food food\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Take that'\n",
    "next_words = 75\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = model.predict_classes(token_list, verbose=0)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
